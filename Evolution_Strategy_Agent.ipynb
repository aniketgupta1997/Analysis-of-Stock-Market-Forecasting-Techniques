{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Evolution Strategy Agent.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTF5GlpXL-xv"
      },
      "source": [
        "##Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNOymAFBDaBu"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "sns.set()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9H1jnDPMDQl"
      },
      "source": [
        "##Supporting Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUmIxIgzDcxr"
      },
      "source": [
        "import pkg_resources\n",
        "import types\n",
        "\n",
        "\n",
        "def get_imports():\n",
        "    for name, val in globals().items():\n",
        "        if isinstance(val, types.ModuleType):\n",
        "            name = val.__name__.split('.')[0]\n",
        "        elif isinstance(val, type):\n",
        "            name = val.__module__.split('.')[0]\n",
        "        poorly_named_packages = {'PIL': 'Pillow', 'sklearn': 'scikit-learn'}\n",
        "        if name in poorly_named_packages.keys():\n",
        "            name = poorly_named_packages[name]\n",
        "        yield name\n",
        "\n",
        "\n",
        "imports = list(set(get_imports()))\n",
        "requirements = []\n",
        "for m in pkg_resources.working_set:\n",
        "    if m.project_name in imports and m.project_name != 'pip':\n",
        "        requirements.append((m.project_name, m.version))\n",
        "\n",
        "for r in requirements:\n",
        "    print('{}=={}'.format(*r))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OSOHSnUMJFS"
      },
      "source": [
        "##Reading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6czHELLRDfjT"
      },
      "source": [
        "df = pd.read_csv('/content/APOLLOTYRE.NS.csv')\n",
        "df = df.fillna(method = 'pad')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOv7yWzwMpog"
      },
      "source": [
        "##Algorithm for updating weights of the Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdPDdzMWDiy7"
      },
      "source": [
        "class Deep_Evolution_Strategy:\n",
        "\n",
        "    inputs = None\n",
        "\n",
        "    def __init__(\n",
        "        self, weights, reward_function, population_size, sigma, learning_rate\n",
        "    ):\n",
        "        self.weights = weights\n",
        "        self.reward_function = reward_function\n",
        "        self.population_size = population_size\n",
        "        self.sigma = sigma\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def _get_weight_from_population(self, weights, population):\n",
        "        weights_population = []\n",
        "        for index, i in enumerate(population):\n",
        "            jittered = self.sigma * i\n",
        "            weights_population.append(weights[index] + jittered)\n",
        "        return weights_population\n",
        "\n",
        "    def get_weights(self):\n",
        "        return self.weights\n",
        "\n",
        "    def train(self, epoch = 100, print_every = 1):\n",
        "        lasttime = time.time()\n",
        "        for i in range(epoch):\n",
        "            population = []\n",
        "            rewards = np.zeros(self.population_size)\n",
        "            for k in range(self.population_size):\n",
        "                x = []\n",
        "                for w in self.weights:\n",
        "                    x.append(np.random.randn(*w.shape))\n",
        "                population.append(x)\n",
        "            for k in range(self.population_size):\n",
        "                weights_population = self._get_weight_from_population(\n",
        "                    self.weights, population[k]\n",
        "                )\n",
        "                rewards[k] = self.reward_function(weights_population)\n",
        "            rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-7)\n",
        "            for index, w in enumerate(self.weights):\n",
        "                A = np.array([p[index] for p in population])\n",
        "                self.weights[index] = (\n",
        "                    w\n",
        "                    + self.learning_rate\n",
        "                    / (self.population_size * self.sigma)\n",
        "                    * np.dot(A.T, rewards).T\n",
        "                )\n",
        "            if (i + 1) % print_every == 0:\n",
        "                print(\n",
        "                    'iter %d. reward: %f'\n",
        "                    % (i + 1, self.reward_function(self.weights))\n",
        "                )\n",
        "        print('time taken to train:', time.time() - lasttime, 'seconds')\n",
        "\n",
        "\n",
        "class Model:\n",
        "    def __init__(self, input_size, layer_size, output_size):\n",
        "        self.weights = [\n",
        "            np.random.randn(input_size, layer_size),\n",
        "            np.random.randn(layer_size, output_size),\n",
        "            np.random.randn(1, layer_size),\n",
        "        ]\n",
        "\n",
        "    def predict(self, inputs):\n",
        "        feed = np.dot(inputs, self.weights[0]) + self.weights[-1]\n",
        "        decision = np.dot(feed, self.weights[1])\n",
        "        return decision\n",
        "\n",
        "    def get_weights(self):\n",
        "        return self.weights\n",
        "\n",
        "    def set_weights(self, weights):\n",
        "        self.weights = weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhMGjAolM31T"
      },
      "source": [
        "##Evolution Strategy Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHiclV0CDlk8"
      },
      "source": [
        "class Agent:\n",
        "\n",
        "    POPULATION_SIZE = 15\n",
        "    SIGMA = 0.1\n",
        "    LEARNING_RATE = 0.03\n",
        "\n",
        "    def __init__(self, model, window_size, trend, skip, initial_money):\n",
        "        self.model = model\n",
        "        self.window_size = window_size\n",
        "        self.half_window = window_size // 2\n",
        "        self.trend = trend\n",
        "        self.skip = skip\n",
        "        self.initial_money = initial_money\n",
        "        self.es = Deep_Evolution_Strategy(\n",
        "            self.model.get_weights(),\n",
        "            self.get_reward,\n",
        "            self.POPULATION_SIZE,\n",
        "            self.SIGMA,\n",
        "            self.LEARNING_RATE,\n",
        "        )\n",
        "    def change_data(self, trend):\n",
        "        self.trend = trend\n",
        "    \n",
        "\n",
        "    def act(self, sequence):\n",
        "        decision = self.model.predict(np.array(sequence))\n",
        "        return np.argmax(decision[0])\n",
        "    \n",
        "    def get_state(self, t):\n",
        "        window_size = self.window_size + 1\n",
        "        d = t - window_size + 1\n",
        "        block = self.trend[d : t + 1] if d >= 0 else -d * [self.trend[0]] + self.trend[0 : t + 1]\n",
        "        res = []\n",
        "        for i in range(window_size - 1):\n",
        "            res.append(block[i + 1] - block[i])\n",
        "        return np.array([res])\n",
        "\n",
        "    def get_reward(self, weights):\n",
        "        initial_money = self.initial_money\n",
        "        starting_money = initial_money\n",
        "        self.model.weights = weights\n",
        "        state = self.get_state(0)\n",
        "        inventory = []\n",
        "        quantity = 0\n",
        "        for t in range(0, len(self.trend) - 1, self.skip):\n",
        "            action = self.act(state)\n",
        "            next_state = self.get_state(t + 1)\n",
        "            \n",
        "            if action == 1 and starting_money >= self.trend[t]:\n",
        "                inventory.append(self.trend[t])\n",
        "                starting_money -= close[t]\n",
        "                \n",
        "            elif action == 2 and len(inventory):\n",
        "                bought_price = inventory.pop(0)\n",
        "                starting_money += self.trend[t]\n",
        "\n",
        "            state = next_state\n",
        "        return ((starting_money - initial_money) / initial_money) * 100\n",
        "\n",
        "    def fit(self, iterations, checkpoint):\n",
        "        self.es.train(iterations, print_every = checkpoint)\n",
        "\n",
        "    def buy(self):\n",
        "        initial_money = self.initial_money\n",
        "        state = self.get_state(0)\n",
        "        starting_money = initial_money\n",
        "        states_sell = []\n",
        "        states_buy = []\n",
        "        inventory = []\n",
        "        for t in range(0, len(self.trend) - 1, self.skip):\n",
        "            action = self.act(state)\n",
        "            next_state = self.get_state(t + 1)\n",
        "            \n",
        "            if action == 1 and initial_money >= self.trend[t]:\n",
        "                inventory.append(self.trend[t])\n",
        "                initial_money -= self.trend[t]\n",
        "                states_buy.append(t)\n",
        "                print('day %d: buy 1 unit at price %f, total balance %f'% (t, self.trend[t], initial_money))\n",
        "            \n",
        "            elif action == 2 and len(inventory):\n",
        "                bought_price = inventory.pop(0)\n",
        "                initial_money += self.trend[t]\n",
        "                states_sell.append(t)\n",
        "                try:\n",
        "                    invest = ((close[t] - bought_price) / bought_price) * 100\n",
        "                except:\n",
        "                    invest = 0\n",
        "                print(\n",
        "                    'day %d, sell 1 unit at price %f, investment %f %%, total balance %f,'\n",
        "                    % (t, close[t], invest, initial_money)\n",
        "                )\n",
        "            state = next_state\n",
        "\n",
        "        invest = ((initial_money - starting_money) / starting_money) * 100\n",
        "        total_gains = initial_money - starting_money\n",
        "        return states_buy, states_sell, total_gains, invest"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjnGmfgqM_Tg"
      },
      "source": [
        "##Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTsdYostDoqb"
      },
      "source": [
        "close = df.Close.values.tolist()\n",
        "window_size = 30\n",
        "skip = 1\n",
        "initial_money = 2500\n",
        "\n",
        "model = Model(input_size = window_size, layer_size = 500, output_size = 3)\n",
        "agent = Agent(model = model, \n",
        "              window_size = window_size,\n",
        "              trend = close,\n",
        "              skip = skip,\n",
        "              initial_money = initial_money)\n",
        "agent.fit(iterations = 500, checkpoint = 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Xmv7hbUNFU5"
      },
      "source": [
        "##Testing the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkAHQOK6Dqgj"
      },
      "source": [
        "states_buy, states_sell, total_gains, invest = agent.buy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAcxw4SCNKjW"
      },
      "source": [
        "##Visualizing the resluts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCLYkJwVEAZw"
      },
      "source": [
        "fig = plt.figure(figsize = (15,5))\n",
        "plt.plot(close, color='r', lw=2.)\n",
        "plt.plot(close, '^', markersize=10, color='m', label = 'buying signal', markevery = states_buy)\n",
        "plt.plot(close, 'v', markersize=10, color='k', label = 'selling signal', markevery = states_sell)\n",
        "plt.title('Apollo Tyre- total investment %d, total gains %.2f, gains percent %.2f%%'%(initial_money, total_gains, invest))\n",
        "plt.legend()\n",
        "plt.xlabel(\"Days\")\n",
        "plt.ylabel(\"Close Price (in ruppees)\")\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bb19gLqhEDUT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}